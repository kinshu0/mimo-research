{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.models as models\n",
    "from keras.layers import Dense, Dropout\n",
    "# from keras.layers.core import Dense,Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(In_train, Out_train, In_test, Out_test,\n",
    "          epochs, batch_size,dr,\n",
    "          num_hidden_layers, nodes_per_layer,\n",
    "          loss_fn,n_BS,n_beams):\n",
    "    \n",
    "    in_shp = list(In_train.shape[1:])\n",
    "\n",
    "    AP_models = []\n",
    "    for bs_idx in range(n_BS):\n",
    "        idx_str = 'BS%i' % bs_idx\n",
    "        idx = bs_idx*n_beams\n",
    "        \n",
    "        model = models.Sequential()\n",
    "        model.add(Dense(nodes_per_layer, activation='relu', kernel_initializer='he_normal', input_shape=in_shp))\n",
    "        model.add(Dropout(dr))\n",
    "        for h in range(num_hidden_layers):\n",
    "            model.add(Dense(nodes_per_layer, activation='relu', kernel_initializer='he_normal'))\n",
    "            model.add(Dropout(dr))\n",
    "        \n",
    "        model.add(Dense(n_beams, activation='relu', kernel_initializer='he_normal',\n",
    "                  name=\"dense\" + idx_str + \"o\"))\n",
    "        model.compile(loss=loss_fn, optimizer='adam')\n",
    "        model.summary()\n",
    "    \n",
    "        model.fit(In_train,\n",
    "                    Out_train[:, idx:idx + n_beams],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(In_test, Out_test[:,idx:idx + n_beams]),\n",
    "                    callbacks = [\n",
    "                        #keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "                        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "                    ])\n",
    "        \n",
    "        AP_models.append(model)\n",
    "        \n",
    "        \n",
    "    return AP_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input and output sets generated from MATLAB\n",
    "In_set_file=loadmat('DLCB_dataset/DLCB_input.mat')\n",
    "Out_set_file=loadmat('DLCB_dataset/DLCB_output.mat')\n",
    "\n",
    "In_set=In_set_file['DL_input']\n",
    "Out_set=Out_set_file['DL_output']\n",
    "\n",
    "# Parameter initialization\n",
    "num_user_tot=In_set.shape[0]\n",
    "n_DL_size=[0.001, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7]\n",
    "count=0\n",
    "num_tot_TX=4\n",
    "num_beams=512\n",
    "\n",
    "for DL_size_ratio in n_DL_size:\n",
    "    \n",
    "    print (DL_size_ratio)\n",
    "    count=count+1\n",
    "    DL_size=int(num_user_tot*DL_size_ratio)\n",
    "    \n",
    "    np.random.seed(2016)\n",
    "    n_examples = DL_size\n",
    "    num_train  = int(DL_size * 0.8)\n",
    "    num_test   = int(num_user_tot*.2)\n",
    "    \n",
    "    train_index = np.random.choice(range(0,num_user_tot), size=num_train, replace=False)\n",
    "    rem_index = set(range(0,num_user_tot))-set(train_index)\n",
    "    test_index= list(set(np.random.choice(list(rem_index), size=num_test, replace=False)))\n",
    "    \n",
    "    In_train = In_set[train_index]\n",
    "    In_test =  In_set[test_index] \n",
    "        \n",
    "    Out_train = Out_set[train_index]\n",
    "    Out_test = Out_set[test_index]\n",
    "    \n",
    "    \n",
    "    # Learning model parameters\n",
    "    epochs = 10     \n",
    "    batch_size = 100  \n",
    "    dr = 0.05                  # dropout rate  \n",
    "    num_hidden_layers=4\n",
    "    nodes_per_layer=In_train.shape[1]\n",
    "    loss_fn='mean_squared_error'\n",
    "    \n",
    "    # Model training\n",
    "    AP_models = train(In_train, Out_train, In_test, Out_test,\n",
    "                                          epochs, batch_size,dr,\n",
    "                                          num_hidden_layers, nodes_per_layer,\n",
    "                                          loss_fn,num_tot_TX,num_beams)\n",
    "\n",
    "    \n",
    "    # Model running/testing\n",
    "    DL_Result={}\n",
    "    for idx in range(0,num_tot_TX,1): \n",
    "        beams_predicted=AP_models[idx].predict( In_test, batch_size=10, verbose=0)\n",
    "    \n",
    "        DL_Result['TX'+str(idx+1)+'Pred_Beams']=beams_predicted\n",
    "        DL_Result['TX'+str(idx+1)+'Opt_Beams']=Out_test[:,idx*num_beams:(idx+1)*num_beams]\n",
    "\n",
    "    DL_Result['user_index']=test_index\n",
    "    \n",
    "    \n",
    "    if not os.path.exists('./DLCB_code_output'):\n",
    "                          os.makedirs('DLCB_code_output')\n",
    "    savemat('DLCB_code_output/DL_Result'+str(count)+'.mat',DL_Result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(glob.glob('DLCB_code_output/DL_Result*'), key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "num_files = len(file_list)\n",
    "\n",
    "user_index = []\n",
    "pred_beams = []\n",
    "opt_beams = []\n",
    "for file in tqdm(file_list, desc='Reading DL results'):\n",
    "    matfile = loadmat(file)\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for idx in range(num_bs):\n",
    "        l1.append(matfile['TX'+str(idx+1)+'Pred_Beams'])\n",
    "        l2.append(matfile['TX'+str(idx+1)+'Opt_Beams'])\n",
    "        \n",
    "    pred_beams.append(l1)\n",
    "    opt_beams.append(l2)\n",
    "    user_index.append(matfile['user_index'])\n",
    "\n",
    "\n",
    "Pn = -204 + 10*np.log10(BW) # Noise power in dB\n",
    "SNR = 10**(.1*(0-Pn))\n",
    "\n",
    "ach_rate_DL = np.zeros(num_files)\n",
    "ach_rate_opt = np.zeros(num_files)\n",
    "\n",
    "eff_rate = np.zeros(num_files)\n",
    "opt_rate = np.zeros(num_files)\n",
    "for file_idx in tqdm(np.arange(num_files), desc = 'Calculating results'):\n",
    "    user_index_file = user_index[file_idx].flatten()\n",
    "    for ue_idx in range(len(user_index_file)):\n",
    "        eff_ch = []\n",
    "        opt_ch = []\n",
    "        for bs_idx in range(num_bs):\n",
    "            if file_idx == 0: # Random BF - 0 Samples\n",
    "                pred_beam_idx = np.random.randint(num_beams)\n",
    "            else:\n",
    "                pred_beam_idx = np.argmax(pred_beams[file_idx][bs_idx][ue_idx])\n",
    "            opt_beam_idx = np.argmax(opt_beams[file_idx][bs_idx][ue_idx])\n",
    "            ch_single_bs = dataset[bs_idx]['user']['channel'][user_index_file[ue_idx]].squeeze()\n",
    "            eff_ch_single_pred = ch_single_bs.T.conj() @ F[:, pred_beam_idx]\n",
    "            opt_ch_single_pred = ch_single_bs.T.conj() @ F[:, opt_beam_idx]\n",
    "            eff_ch.append(eff_ch_single_pred)\n",
    "            opt_ch.append(opt_ch_single_pred)\n",
    "        eff_ch = np.array(eff_ch)\n",
    "        opt_ch = np.array(opt_ch)\n",
    "        eff_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(eff_ch.conj().T @ eff_ch))))\n",
    "        opt_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(opt_ch.conj().T @ opt_ch))))\n",
    "    eff_rate[file_idx] /= len(user_index_file)*num_OFDM\n",
    "    opt_rate[file_idx] /= len(user_index_file)*num_OFDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_user=(102/parameters['bs_antenna'][0]['shape'][1])*np.pi/180\n",
    "alpha=60*np.pi/180\n",
    "distance_user=10\n",
    "Tc_const=(distance_user*theta_user)/(2*np.sin(alpha)) # ms\n",
    "Tt=10*1e-6; # ms\n",
    "\n",
    "v_mph=50\n",
    "v=v_mph*1000*1.6/3600 # m/s\n",
    "Tc=Tc_const/v\n",
    "\n",
    "overhead_opt=1-(num_beams*Tt)/Tc # overhead of beam training\n",
    "overhead_DL=1-Tt/Tc # overhead of proposed DL method\n",
    "\n",
    "#%% Plotting the figure\n",
    "DL_size_array=np.arange(0, 2.5*(num_files), 2.5);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(DL_size_array, opt_rate, '--k', label = 'Genie-aided Coordinated Beamforming')\n",
    "plt.plot(DL_size_array, eff_rate*overhead_DL, '-bo', label = 'Deep Learning Coordinated Beamforming')\n",
    "plt.plot(DL_size_array, opt_rate*overhead_opt, '-rs', label = 'Baseline Coordinated Beamforming')\n",
    "plt.ylim([0, 6])\n",
    "plt.minorticks_on()\n",
    "plt.grid()\n",
    "plt.xlabel('Deep Learning Dataset Size (Thousand Samples)')\n",
    "plt.ylabel('Achievable Rate (bps/Hz)')\n",
    "plt.legend()\n",
    "plt.savefig('result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
